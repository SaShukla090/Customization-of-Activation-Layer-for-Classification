# Customization-of-Activation-Layer-for-Classification
Activation functions are crucial for modelling complex data patterns in deep learning. This study adapts traditional functions (Sigmoid, Tanh etc) by adjusting a few parameters for faster convergence and better performance. Tests on various models confirm the effectiveness of the method. 
